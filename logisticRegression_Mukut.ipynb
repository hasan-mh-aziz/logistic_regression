{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "from numpy.linalg import inv,pinv\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import math\n",
    "%run common_functions.ipynb\n",
    "%run naive_bayes_classifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, cost_thresold = 0.001):\n",
    "        self.cost_thresold = cost_thresold\n",
    "    \n",
    "    def add_ones(self, X):\n",
    "        return np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, x, theta):\n",
    "        z = np.dot(x, theta.T)\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def findCost(self, theta, X, y):\n",
    "        m = len(y)\n",
    "        temp = self.sigmoid(X, theta)\n",
    "        total_cost = np.multiply(y, np.log(temp)) + np.multiply(1 - y, np.log(1 - temp))\n",
    "        return -np.mean(total_cost)\n",
    "\n",
    "\n",
    "    def findGradient(self, theta, X, y):\n",
    "        m = len(y)\n",
    "        return (1 / m) * np.dot(X.T, (self.sigmoid(X, theta) - y))\n",
    "    \n",
    "    \n",
    "    def gradientDescent(self, class_index, X, y, learning_rate=0.1, maximum_iteration=5000):\n",
    "        class_theta = self.theta[class_index]\n",
    "        costs = []\n",
    "\n",
    "        for i in range(maximum_iteration):\n",
    "            gradient = self.findGradient(class_theta, X, y)\n",
    "            cost = self.findCost(class_theta, X, y)\n",
    "            class_theta -= learning_rate * gradient\n",
    "            costs.append(cost)\n",
    "                \n",
    "            if abs(cost) < self.cost_thresold:\n",
    "#                 print(\"loop broke by thresold\")\n",
    "                break\n",
    "\n",
    "        return costs, class_theta\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, learning_rate=0.1, maximum_iteration=5000):\n",
    "        X = self.add_ones(X)\n",
    "            \n",
    "        (m, n) = X.shape\n",
    "        self.class_labels = np.unique(y)\n",
    "        k = len(self.class_labels)\n",
    "        self.theta = np.zeros((k, n))\n",
    "        class_costs = {}\n",
    "\n",
    "#         Initial call to print 0% progress\n",
    "        printProgressBar(0, k, prefix = 'Logistic Regression Progress:', suffix = 'Complete', length = 50)\n",
    "        \n",
    "        for class_label_index in range(k):\n",
    "            printProgressBar(class_label_index + 1, k, prefix = 'Logistic Regression Progress:', suffix = 'Complete', length = 50)\n",
    "            one_vs_all_class = (y == self.class_labels[class_label_index]).flatten()\n",
    "            costs, self.theta[class_label_index] = self.gradientDescent(class_label_index, X, one_vs_all_class,learning_rate, maximum_iteration)\n",
    "            class_costs[class_label_index] = costs\n",
    "            \n",
    "        return class_costs\n",
    "\n",
    "    \n",
    "    def make_predictions(self, X, y):\n",
    "        X = self.add_ones(X)\n",
    "\n",
    "        predictions = self.class_labels[np.argmax(self.sigmoid(X, self.theta), axis = 1)]\n",
    "        error_percentage = 100 - np.mean(predictions == y.flatten()) * 100\n",
    "        \n",
    "        return error_percentage, predictions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (1797, 64)\n",
      "y:  (1797, 1)\n",
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "with open('digits.csv', 'r') as csvfile:\n",
    "    digitDataset = np.asarray(list(csv.reader(csvfile, quoting=csv.QUOTE_NONNUMERIC)))\n",
    "    \n",
    "x = digitDataset[:, :-1]\n",
    "y = digitDataset[:, -1:]\n",
    "print(\"x: \", x.shape)\n",
    "print(\"y: \", y.shape)\n",
    "print(np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Logistic regression. Default maximum iteration for gradient descent  is 5000\n",
    "# and thresold for cost is 0.001\n",
    "lg = LogisticRegression()\n",
    "# # initialize Naive Bayes\n",
    "# nb = naive_bayes_classifier()\n",
    "\n",
    "# # k-fold cross validaion\n",
    "# k = 10\n",
    "# # get the train and test indices for k-fold cross validation\n",
    "# k_folded_train_indices, k_folded_test_indices = get_k_fold_indices(k, y.shape[0], shuffle=True)\n",
    "\n",
    "# folds_errors = []\n",
    "\n",
    "# for fold, train_indices in enumerate(k_folded_train_indices):\n",
    "#     print(f'Fold#{fold + 1} of {k}:')\n",
    "#     test_indices = k_folded_test_indices[fold]\n",
    "    \n",
    "#     train_x = x[train_indices]\n",
    "#     test_x = x[test_indices]\n",
    "#     train_y = y[train_indices]\n",
    "#     test_y = y[test_indices]\n",
    "    \n",
    "#     class_costs = lg.fit(train_x, train_y)\n",
    "#     nb.fit(train_x, train_y)\n",
    "\n",
    "#     train_error, train_predictions = lg.make_predictions(train_x, train_y)\n",
    "#     test_error, test_predictions = lg.make_predictions(test_x, test_y)\n",
    "#     nb_train_accuracy, nb_train_predictions = nb.predict(train_x, train_y)\n",
    "#     nb_test_accuracy, nb_test_predictions = nb.predict(test_x, test_y)\n",
    "# #     print(train_accuracy)\n",
    "#     folds_errors.append([train_error, test_error, 100 - nb_train_accuracy, 100 - nb_test_accuracy])\n",
    "\n",
    "# errors_df = pd.DataFrame(folds_errors, columns = [\"LG Train errors(%)\", \"LG Test errors(%)\", \"NB rain errors(%)\", \"NB Test errors(%)\"])\n",
    "# errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds_errors = np.asarray(folds_errors)\n",
    "# labels = [\"lg_train\", \"lg_test\", \"nb_train\", \"nb_test\"]\n",
    "\n",
    "# for i in range(folds_errors.shape[1]):\n",
    "#     plt.plot(folds_errors[:, i], label=labels[i])\n",
    "    \n",
    "# plt.xlabel(\"#fold\")\n",
    "# plt.ylabel(\"Error(%)\")\n",
    "# plt.title(f\"Logistic regression and Naive Bayes Classifier on {k}-fold cross vaidation\")\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Logistic Regression Progress: |--------------------------------------------------| 0.0% Complete\r",
      "\r",
      "Logistic Regression Progress: |█████---------------------------------------------| 10.0% Complete\r",
      "\r",
      "Logistic Regression Progress: |██████████----------------------------------------| 20.0% Complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasan/Installations/miniconda3/envs/AML/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "/home/hasan/Installations/miniconda3/envs/AML/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Progress: |██████████████████████████████████████████████████| 100.0% Complete\n",
      "   LG Train errors(%)  LG Test errors(%)\n",
      "0             1.32128           3.342618\n"
     ]
    }
   ],
   "source": [
    "# 80-20 split\n",
    "# train_indices, test_indices = get_train_test_indices_by_train_percentage(80, y.shape[0], shuffle=True)\n",
    "# train_x = x[train_indices]\n",
    "# test_x = x[test_indices]\n",
    "# train_y = y[train_indices]\n",
    "# test_y = y[test_indices]\n",
    "\n",
    "# class_costs = lg.fit(train_x, train_y)\n",
    "\n",
    "# split_train_error, split_train_predictions = lg.make_predictions(train_x, train_y)\n",
    "# split_test_error, split_test_predictions = lg.make_predictions(test_x, test_y)\n",
    "\n",
    "# split_errors = [[split_train_error, split_test_error]]\n",
    "\n",
    "# split_errors_df = pd.DataFrame(split_errors, columns = [\"LG Train errors(%)\", \"LG Test errors(%)\"])\n",
    "# print(split_errors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
